## **2. Configuração do Ambiente Local**

### Adicional sobre Hardware (Configuração Multi-GPU)
#### **Configuração Multi-GPU**
Para usuários que desejam utilizar múltiplas GPUs para melhorar o desempenho de modelos maiores, como o LLaMA, siga este exemplo usando PyTorch:
```python
import torch
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = model.to(device)
if torch.cuda.device_count() > 1:
    model = torch.nn.DataParallel(model)
```
Esse código garante que o modelo seja executado em várias GPUs quando disponível.

Para modelos maiores como o LLaMA (13B ou 65B), recomenda-se uma configuração com múltiplas GPUs para garantir desempenho consistente. 
- Exemplos de configurações recomendadas incluem:
  - NVIDIA A100 (20GB ou mais) em configuração multi-GPU.
  - Uso de servidores compatíveis com escalonamento de GPUs, como os disponíveis em AWS, GCP ou Azure.

Este passo envolve preparar o servidor local para rodar o modelo LLaMA, configurar dependências e preparar o ambiente para a integração com o WhatsApp.


#### **No Windows:**
Se estiver usando o **Windows Subsystem for Linux (WSL2)**:
```bash
sudo apt update && sudo apt upgrade -y
```

Se estiver rodando nativamente, atualize seu sistema pelo Windows Update.
.


### **2.2. Instalação das Dependências Necessárias**

Certifique-se de instalar as ferramentas básicas.

#### **Python e Pip**
Python será necessário para executar o modelo LLaMA e scripts do chatbot.

1. **Instale o Python:**
   ```bash
   sudo apt install python3 python3-pip -y
   ```

2. **Verifique a versão instalada:**
   ```bash
   python3 --version
   ```

3. **Atualize o Pip:**
   ```bash
   pip install --upgrade pip
   ```



#### **Gerenciador de Pacotes Opcional ao PIP (Conda)**

Caso deseje, você pode instalar o Conda. Uma alternativa ao Pip, útil para gerenciar ambientes isolados.

1. Baixe o **Miniconda**:
   ```bash
   wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh
   bash Miniconda3-latest-Linux-x86_64.sh
   ```

2. Crie um ambiente virtual para o LLaMA:
   ```bash
   conda create -n llama_env python=3.8
   conda activate llama_env
   ```



#### **Docker (Para Ambiente Isolado)**
#### **Teste do Chatbot em Ambiente Docker**
#### **Dockerfile Completo**
Certifique-se de adicionar o comando CMD para iniciar o chatbot automaticamente no Docker:
```dockerfile
CMD ["python", "app.py"]
```
Isso garante que o contêiner inicie o servidor do chatbot sem configurações manuais adicionais.

Antes da implantação, teste o chatbot em um ambiente Docker para verificar conflitos. Use o comando:
```bash
docker build -t chatbot .
docker run -p 5000:5000 chatbot
```
Depois, simule interações para validar a integração e funcionamento do sistema.


O Docker facilita a instalação de dependências, criando um ambiente limpo e padronizado.

1. **Instale o Docker:**
   ```bash
   sudo apt install docker.io -y
   sudo systemctl enable --now docker
   ```

2. **Verifique a instalação:**
   ```bash
   docker --version
   ```

3. **Instale o Docker Compose (opcional):**
   ```bash
   sudo curl -L "https://github.com/docker/compose/releases/latest/download/docker-compose-$(uname -s)-$(uname -m)" -o /usr/local/bin/docker-compose
   sudo chmod +x /usr/local/bin/docker-compose
   docker-compose --version
   ```

4. **Configure Permissões para o Docker:**
   ```bash
   sudo usermod -aG docker $USER
   ```

.

### **2.3. Instalação do LLaMA**
#### **Verifique se o modelo LLaMA foi baixado corretamente**
#### **Permissões para Modelos Pré-Treinados**
Após baixar os modelos do repositório oficial, configure as permissões adequadas no diretório:
```bash
chmod -R 755 ~/models/llama
```
Isso evita problemas de acesso ao carregar o modelo no ambiente de execução.

Certifique-se de que o modelo foi extraído no diretório correto. Use os comandos:
```bash
ls ~/models/llama
```
Verifique se os arquivos necessários (como `config.json`, `pytorch_model.bin`) estão presentes. Caso contrário, revise o download e extração.


O modelo LLaMA será usado para fornecer respostas inteligentes no chatbot.

1. **Clone o Repositório Oficial:**
   ```bash
   git clone https://github.com/facebookresearch/llama.git
   cd llama
   ```

2. **Instale as Dependências do LLaMA:**
   No diretório do repositório:
   ```bash
   pip install -r requirements.txt
   ```

3. **Baixe o Modelo Pré-Treinado:**
   - Acesse o site oficial da Meta e solicite acesso ao modelo LLaMA.
   - Após aprovação, baixe os arquivos e extraia-os em um diretório no servidor:
     ```bash
     mkdir -p ~/models/llama
     cp /caminho/do/modelo/baixado/* ~/models/llama/
     ```

4. **Teste o Modelo Localmente:**
   Execute um script de teste para garantir que o modelo está funcionando:
   ```python
   from transformers import pipeline

   model_pipeline = pipeline("text-generation", model="~/models/llama")
   print(model_pipeline("Olá, como você está?"))
   ```
.


### **2.4. Setando a API do WhatsApp**
### **Prós e Contras das APIs do WhatsApp**
#### **API Oficial**
- **Prós:**
  - Suporte oficial e estabilidade garantida.
  - Melhor compatibilidade com futuras atualizações do WhatsApp.
  - Permite maior controle sobre a autenticação e segurança.
- **Contras:**
  - Processo burocrático para obtenção e configuração.
  - Custos associados dependendo do uso.

#### **API Não-Oficial (Baileys)**
- **Prós:**
  - Rápido e fácil de configurar.
  - Gratuita e sem necessidade de registro formal.
- **Contras:**
  - Risco de bloqueio de número pelo WhatsApp.
  - Falta de suporte oficial para problemas ou bugs.


#### **API Oficial do WhatsApp Business**
1. **Crie um Aplicativo no Meta Developers:**
   - Acesse o [Meta for Developers](https://developers.facebook.com/).
   - Registre um aplicativo e configure o WhatsApp Business API.

2. **Obtenha Credenciais:**
   - Gere o token de acesso.
   - Configure o webhook com o URL do seu servidor (será configurado no próximo passo).

3. **Teste com o Curl:**
   ```bash
   curl -X POST https://graph.facebook.com/v16.0/WHATSAPP_PHONE_NUMBER_ID/messages \
   -H "Authorization: Bearer ACCESS_TOKEN" \
   -H "Content-Type: application/json" \
   -d '{"messaging_product": "whatsapp", "to": "DESTINATION_PHONE_NUMBER", "type": "text", "text": {"body": "Teste de mensagem!"}}'
   ```

#### **API Não-Oficial (Baileys)**
1. **Instale o Baileys:**
   ```bash
   npm install @adiwajshing/baileys
   ```

2. **Configure o Bot com QR Code:**
   - Crie um script básico para iniciar o bot:
     ```javascript
     const { default: makeWASocket } = require('@adiwajshing/baileys');

     const startBot = async () => {
       const sock = makeWASocket();
       sock.ev.on('connection.update', (update) => {
         if (update.qr) {
           console.log('Escaneie este QR code:', update.qr);
         }
       });
     };

     startBot();
     ```

   - Execute o script e escaneie o QR Code com o WhatsApp.

3. **Teste Envio de Mensagens:**
   Após conectar, envie mensagens pelo bot:
   ```javascript
   sock.sendMessage('DESTINATION_PHONE_NUMBER@s.whatsapp.net', { text: 'Olá, isso é um teste!' });
   ```

.

### **2.5. Teste do Ambiente**

Certifique-se de que todas as dependências estão funcionando:
1. Teste o Python:
   ```bash
   python3 --version
   ```
2. Teste o Node.js:
   ```bash
   node -v
   ```
3. Teste o Docker (se instalado):
   ```bash
   docker run hello-world
   ```
4. Teste a Conexão com a API do WhatsApp usando o Curl ou o Baileys.

